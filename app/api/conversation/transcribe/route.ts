/**
 * éŸ³å£°æ–‡å­—èµ·ã“ã—API
 * éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
 * OpenAI Whisper APIãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ãƒ¢ãƒƒã‚¯å®Ÿè£…ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
 */

import { NextRequest, NextResponse } from "next/server";
import { openai } from "@/lib/openai";
import { mockTranscribeAudio } from "@/lib/mock-ocr";
import { requireAuth } from "@/lib/auth-utils";

export async function POST(request: NextRequest) {
  try {
    // èªè¨¼ãƒã‚§ãƒƒã‚¯
    await requireAuth();

    const formData = await request.formData();
    const file = formData.get("file") as File;

    if (!file) {
      return NextResponse.json({ error: "ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“" }, { status: 400 });
    }

    // ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã®ãƒã‚§ãƒƒã‚¯
    const validAudioTypes = [
      "audio/mpeg",
      "audio/mp3",
      "audio/mp4",
      "audio/x-m4a",
      "audio/wav",
      "audio/webm",
      "audio/flac",
    ];
    if (!validAudioTypes.includes(file.type)) {
      return NextResponse.json(
        { error: "å¯¾å¿œã—ã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™ã€‚MP3ã€M4Aã€WAVã€WebMã€FLACã®ã¿å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚" },
        { status: 400 }
      );
    }

    // ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®ãƒã‚§ãƒƒã‚¯ï¼ˆ25MBåˆ¶é™ - Whisper APIã®åˆ¶é™ï¼‰
    const maxSize = 25 * 1024 * 1024;
    if (file.size > maxSize) {
      return NextResponse.json({ error: "ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã¾ã™ï¼ˆæœ€å¤§25MBï¼‰" }, { status: 400 });
    }

    // ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã®ãƒã‚§ãƒƒã‚¯
    const useMock = process.env.USE_MOCK_TRANSCRIBE === "true";

    let transcribedText = "";
    let isMock = false;

    if (useMock) {
      // ãƒ¢ãƒƒã‚¯éŸ³å£°æ–‡å­—èµ·ã“ã—å‡¦ç†
      console.log("ğŸ”§ ãƒ¢ãƒƒã‚¯éŸ³å£°æ–‡å­—èµ·ã“ã—å‡¦ç†ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™");
      transcribedText = await mockTranscribeAudio(file.name);
      isMock = true;
    } else {
      try {
        // OpenAI Whisper APIã§éŸ³å£°æ–‡å­—èµ·ã“ã—
        console.log(`ğŸ¤ éŸ³å£°æ–‡å­—èµ·ã“ã—é–‹å§‹: ${file.name} (${(file.size / 1024 / 1024).toFixed(2)}MB)`);
        
        const response = await openai.audio.transcriptions.create({
          file: file,
          model: "whisper-1",
          language: "ja", // æ—¥æœ¬èªã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
          response_format: "text", // ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§å–å¾—
        });

        transcribedText = response as unknown as string;
        console.log("âœ… éŸ³å£°æ–‡å­—èµ·ã“ã—å®Œäº†");
      } catch (apiError: any) {
        // OpenAI APIã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ¢ãƒƒã‚¯å®Ÿè£…ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        console.warn("âš ï¸ OpenAI APIã‚¨ãƒ©ãƒ¼ã€ãƒ¢ãƒƒã‚¯å®Ÿè£…ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™:", apiError.message);
        
        // ã‚¯ã‚©ãƒ¼ã‚¿ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ç‰¹åˆ¥ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        if (apiError.status === 429 || apiError.code === "insufficient_quota") {
          console.log("ğŸ’¡ ãƒ’ãƒ³ãƒˆ: .env.local ã« USE_MOCK_TRANSCRIBE=true ã‚’è¨­å®šã™ã‚‹ã¨ã€å¸¸ã«ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œã—ã¾ã™");
        }
        
        transcribedText = await mockTranscribeAudio(file.name);
        isMock = true;
      }
    }

    return NextResponse.json({
      success: true,
      text: transcribedText,
      fileName: file.name,
      isMock, // ãƒ¢ãƒƒã‚¯ä½¿ç”¨ã®æœ‰ç„¡ã‚’ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã«é€šçŸ¥
    });
  } catch (error) {
    console.error("éŸ³å£°æ–‡å­—èµ·ã“ã—å‡¦ç†ã‚¨ãƒ©ãƒ¼:", error);
    return NextResponse.json(
      { error: "éŸ³å£°æ–‡å­—èµ·ã“ã—å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ" },
      { status: 500 }
    );
  }
}

